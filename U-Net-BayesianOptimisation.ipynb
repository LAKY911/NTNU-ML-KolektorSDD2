{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net with Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORK IN PROGRESS !! Not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.util import random_noise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "from keras import layers, metrics\n",
    "\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"24_04_22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    if i == 1:\n",
    "      plt.imshow(display_list[i], cmap='gray',  interpolation='nearest')\n",
    "    elif i == 2:\n",
    "      plt.imshow(display_list[i], cmap='jet',  interpolation='nearest')\n",
    "    else:\n",
    "      plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image dimensions, seems to work with non-square inputs\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "IMAGE_HEIGHT =  192\n",
    "IMAGE_WIDTH = 64\n",
    "\n",
    "seed = 4\n",
    "np.random.seed = seed\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "DATA_TRAIN = \"./datasets/KolektorSDD2/train/\"\n",
    "DATA_TEST = \"./datasets/KolektorSDD2/test/\"\n",
    "\n",
    "\n",
    "\n",
    "train_ids = next(os.walk(os.path.join(DATA_TRAIN, \"images/\")))[2]\n",
    "test_ids = next(os.walk(os.path.join(DATA_TEST, \"images/\")))[2]\n",
    "\n",
    "damaged = [] # prepare for data augmentation\n",
    "damaged_mask = []\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "y_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "print('Resizing training images and masks')\n",
    "\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n",
    "    path = DATA_TRAIN \n",
    "\n",
    "    img = imread(path + 'images/' + id_)[:,:,:IMAGE_CHANNELS]  \n",
    "    img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "    img /= 255.0\n",
    "    X_train[n] = img  #Fill empty X_train with values from img\n",
    "\n",
    "    mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "    mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "    mask = imread(mask_file)[:,:]\n",
    "\n",
    "    mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "    mask /= 255.0  \n",
    "    mask = np.where(mask > 0.5, 1.0, 0.0) \n",
    "    y_train[n] = mask \n",
    "\n",
    "    if np.count_nonzero(mask) != 0:\n",
    "        damaged.append(img)\n",
    "        damaged_mask.append(mask)\n",
    "    \n",
    "# test images\n",
    "test_images = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "test_masks = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "sizes_test = []\n",
    "print('Resizing test images') \n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = DATA_TEST\n",
    "    img = imread(path + '/images/' + id_ )[:,:,:IMAGE_CHANNELS]\n",
    "    sizes_test.append([img.shape[0], img.shape[1]])\n",
    "    img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "    img /= 255.0\n",
    "    test_images[n] = img\n",
    "\n",
    "    mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "    mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "    mask = imread(mask_file)[:,:]\n",
    "\n",
    "    mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "    mask /= 255.0     \n",
    "    mask = np.where(mask > 0.5, 1.0, 0.0)   \n",
    "    test_masks[n] = mask \n",
    "\n",
    "## Data augmentation - rotate and flip images\n",
    "vertical_train = np.flip(damaged, axis=0)\n",
    "vertical_test = np.flip(damaged_mask, axis=0)\n",
    "\n",
    "horizontal_train = np.flip(damaged, axis=1)\n",
    "horizontal_test = np.flip(damaged_mask, axis=1)\n",
    "\n",
    "rotating_train = np.rot90(damaged, k=2)\n",
    "rotating_test = np.rot90(damaged_mask, k=2)\n",
    "\n",
    "vert_rot_train = np.rot90(vertical_train, k=2)\n",
    "vert_rot_test = np.rot90(vertical_test, k=2)\n",
    "\n",
    "hor_rot_train = np.rot90(horizontal_train, k=2)\n",
    "hor_rot_test = np.rot90(horizontal_test, k=2)\n",
    "\n",
    "# Done with rotation\n",
    "Boxes = []\n",
    "check = []\n",
    "for img in damaged_mask:\n",
    "    labels = label(img)\n",
    "    regions = regionprops(labels)\n",
    "    if len(regions) == 1:\n",
    "        check.append(1)\n",
    "        for props in regions:\n",
    "            min_x, min_y, max_x, max_y = props.bbox\n",
    "            Boxes.append((min_x, min_y, max_x, max_y))\n",
    "    else:\n",
    "        check.append(0)\n",
    "\n",
    "# Throw out images, which have more than one damage\n",
    "onedamage = [damaged[i] for i in range(len(damaged)) if check[i] == 1]\n",
    "onedamage_mask = [damaged_mask[i] for i in range(len(damaged_mask)) if check[i] == 1]\n",
    "\n",
    "def crop_image(image, bbox):\n",
    "    # Crop the image using NumPy array slicing\n",
    "    cropped_image = image[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "    return cropped_image\n",
    "\n",
    "def overlay_image(background, background_mask, overlay, overlay_mask):\n",
    "    # Generate random position for overlay image\n",
    "    if overlay.shape[1] < background.shape[1]:\n",
    "        x_offset = np.random.randint(0, background.shape[1] - overlay.shape[1])\n",
    "        y_offset = np.random.randint(0, background.shape[0] - overlay.shape[0])\n",
    "    \n",
    "        # Overlay the image\n",
    "        background[y_offset:y_offset + overlay.shape[0], x_offset:x_offset + overlay.shape[1]] = overlay\n",
    "        background_mask[y_offset:y_offset + overlay_mask.shape[0], x_offset:x_offset + overlay_mask.shape[1]] = overlay_mask\n",
    "        return background, background_mask\n",
    "    else:\n",
    "        return background, background_mask\n",
    "\n",
    "generated_img = np.empty((len(onedamage),IMAGE_HEIGHT,IMAGE_WIDTH,3))\n",
    "generated_mask = np.empty((len(onedamage), IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "overlayed_indices = []\n",
    "for i, image in enumerate(onedamage):\n",
    "    # Get the bounding box for the current image\n",
    "    bbox = Boxes[i]\n",
    "    mask = onedamage_mask[i]\n",
    "    # Crop the image\n",
    "    cropped_image = crop_image(image, bbox)\n",
    "    cropped_mask = crop_image(mask, bbox)\n",
    "    # Pick a random overlay image from the list\n",
    "    overlay_image_index = np.random.choice([idx for idx in range(len(X_train)) if idx not in overlayed_indices])\n",
    "    overlay = X_train[overlay_image_index]\n",
    "    overlay_mask = y_train[overlay_image_index]\n",
    "\n",
    "    # Overlay the cropped image onto the random overlay image\n",
    "    new_image, new_mask = overlay_image(overlay, overlay_mask, cropped_image, cropped_mask)\n",
    "    generated_img[i] = new_image\n",
    "    generated_mask[i] = new_mask\n",
    "    overlayed_indices.append(overlay_image_index)\n",
    "\n",
    "noised = np.empty_like(damaged)\n",
    "noised_mask = damaged_mask\n",
    "i = 0\n",
    "for img in damaged:\n",
    "    noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "    noised[i] = noise\n",
    "    i = i+1\n",
    "\n",
    "noised_vert = np.empty_like(damaged)\n",
    "noised_mask_vert = vertical_test\n",
    "i = 0\n",
    "for img in vertical_train:\n",
    "    noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "    noised_vert[i] = noise\n",
    "    i = i+1\n",
    "\n",
    "noised_horr = np.empty_like(damaged)\n",
    "noised_mask_horr = horizontal_test\n",
    "i = 0\n",
    "for img in horizontal_train:\n",
    "    noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "    noised_horr[i] = noise\n",
    "    i = i+1\n",
    "\n",
    "X_train = np.concatenate((vertical_train, horizontal_train, rotating_train, vert_rot_train, hor_rot_train, X_train))\n",
    "y_train = np.concatenate((vertical_test, horizontal_test, rotating_test, vert_rot_test, hor_rot_test, y_train))\n",
    "\n",
    "print('Dataset is ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(learning_rate, channels, batch_size):\n",
    "    # (X_train, y_train) = dataset\n",
    "    global X_train, y_train, test_images, test_masks\n",
    "    non_zero = np.count_nonzero(y_train)\n",
    "    print(non_zero)\n",
    "    # print(non_zero/(IMAGE_HEIGHT*IMAGE_WIDTH*len(y_train))*100)\n",
    "\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"Percentage of faulty images in train data:\", counts[1]/(counts[0]+counts[1])*100, \" %\")\n",
    "    neg = counts[0]\n",
    "    pos = counts[1]\n",
    "   \n",
    "    initial_bias = np.log([pos/neg])\n",
    "    output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = seed)\n",
    "    \n",
    "\n",
    "    #Building U-net model\n",
    "    #Downward stream\n",
    "    inputs = tf.keras.layers.Input((IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
    "    conv_11 = layers.Conv2D(16*channels,kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(inputs)\n",
    "    conv_12 = layers.Conv2D(16*channels,kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_11)#TODO: Understand parameters\n",
    "\n",
    "    max_pool_1 = layers.MaxPool2D((2,2))(conv_12)\n",
    "    conv_21 = layers.Conv2D(32*channels,(3,3),activation = 'relu',padding= 'same',kernel_initializer = 'he_normal')(max_pool_1)\n",
    "    conv_22 = layers.Conv2D(32*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_21)\n",
    "\n",
    "    max_pool_2 = layers.MaxPool2D((2,2))(conv_22)\n",
    "    conv_31 = layers.Conv2D(64*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_2)\n",
    "    conv_32 = layers.Conv2D(64*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_31)\n",
    "\n",
    "    max_pool_3 = layers.MaxPool2D((2,2))(conv_32)\n",
    "    conv_41 = layers.Conv2D(128*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_3)\n",
    "    conv_42 = layers.Conv2D(128*channels,(3,3),activation = 'relu', padding= 'same')(conv_41)\n",
    "\n",
    "    max_pool_4 = layers.MaxPool2D((2,2))(conv_42)\n",
    "    conv_51 = layers.Conv2D(256*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_4)\n",
    "    conv_52 = layers.Conv2D(256*channels,(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_51)\n",
    "\n",
    "    #Upward stream\n",
    "    upconv_1 = layers.Conv2DTranspose(128*channels,(2,2), strides=(2,2))(conv_52)\n",
    "    upconv_1_conc = layers.concatenate([upconv_1,conv_42])\n",
    "    conv_61 = layers.Conv2D(128*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_1_conc)\n",
    "    conv_62 = layers.Conv2D(128*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_61)\n",
    "\n",
    "    upconv_2 = layers.Conv2DTranspose(64*channels, (2,2), strides = (2,2))(conv_62)\n",
    "    upconv_2_conc = layers.concatenate([upconv_2, conv_32])\n",
    "    conv_71 = layers.Conv2D(64*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_2_conc)\n",
    "    conv_72 = layers.Conv2D(64*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_71)\n",
    "\n",
    "    upconv_3 = layers.Conv2DTranspose(32*channels,(2,2), strides=(2,2))(conv_72)\n",
    "    upconv_3_conc = layers.concatenate([upconv_3,conv_22])\n",
    "    conv_81 = layers.Conv2D(32*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_3_conc)\n",
    "    conv_82 = layers.Conv2D(32*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_81)\n",
    "\n",
    "    upconv_4 = layers.Conv2DTranspose(16*channels,(2,2), strides=(2,2))(conv_82)\n",
    "    upconv_4_conc = layers.concatenate([upconv_4,conv_12])\n",
    "    conv_91 = layers.Conv2D(16*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_4_conc)\n",
    "    conv_92 = layers.Conv2D(16*channels,(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_91)\n",
    "    outputs = layers.Conv2D(1,(1,1), activation = 'sigmoid', padding = 'same',kernel_initializer = 'he_normal', bias_initializer=output_bias)(conv_92)#TODO: Check function here\n",
    "\n",
    "    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "    from keras.optimizers import Adam, SGD\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    # optimizerSGD = SGD(learning_rate = learning_rate)\n",
    "\n",
    "    #Compiling model\n",
    "    model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=[metrics.BinaryIoU(threshold=0.5)]) #TODO: Parameters check #metrics.BinaryIoU(), 'accuracy'\n",
    "    # model.compile(optimizer=optimizerSGD, loss='mse', metrics=[metrics.BinaryIoU(\n",
    "                                                            # target_class_ids=[0],\n",
    "                                                            # threshold=0.5)]) #TODO: Parameters check #metrics.BinaryIoU(), 'accuracy'\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, restore_best_weights=True)\n",
    "\n",
    "    epochs = 100  \n",
    "    batch_size = batch_size\n",
    "\n",
    "    # run_name = \"/run-%d\" % session_num\n",
    "    # logdir = f\"logs/hparam_tuning_{date}/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + run_name\n",
    "    # tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (X_val, y_val), \n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        callbacks=[early_stopping],\n",
    "                        batch_size = batch_size,) \n",
    "\n",
    "    # Y_pred = model.predict(test_images)\n",
    "    \n",
    "    # Model Loss\n",
    "    plot_folder = f\"./plots/{date}\"\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(history.history[\"loss\"],label = \"Train Loss\", color = \"black\")\n",
    "    # plt.plot(history.history[\"val_loss\"],label = \"Validation Loss\", color = \"darkred\", marker = \"+\", linestyle=\"dashed\", markeredgecolor = \"purple\", markeredgewidth = 2)\n",
    "    # plt.title(f\"Model Loss - session {session_num}\", color = \"darkred\", size = 13)\n",
    "    # plt.legend()\n",
    "    # plt.savefig(f\"plots/{date}/loss_session_{session_num}.png\")\n",
    "    # plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    # model_folder = f\"./trainedModels/hptuning_{date}\"\n",
    "    # os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    # model.save(f\"trainedModels/hptuning_{date}/hptuning_session{session_num}.h5\")\n",
    "\n",
    "    # Model Jaccard score\n",
    "    threshold_list = np.linspace(0.0, 0.4, 30)\n",
    "    jaccard_scores = []\n",
    "    Y_pred = model.predict(test_images)\n",
    "    true_masks_flat = test_masks.reshape(test_masks.shape[0], -1)\n",
    "    for threshold in threshold_list:\n",
    "        y_pred_binary = (Y_pred >= threshold).astype(int)\n",
    "        pred_masks_binary_flat = y_pred_binary.reshape(y_pred_binary.shape[0], -1)\n",
    "        jaccard_scores.append(jaccard_score(true_masks_flat, pred_masks_binary_flat, average=\"micro\"))\n",
    "    print(\"Best jacard score: \", max(jaccard_scores))\n",
    "    \n",
    "    accuracy = max(jaccard_scores)\n",
    "\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space\n",
    "pbounds = {'learning_rate': (1e-5, 1e-2),\n",
    "           'channels': (1, 2),\n",
    "           'batch_size': (16, 128),\n",
    "        #    'num_filters': (16, 128),\n",
    "        #    'kernel_size': (3, 5),\n",
    "        #    'pool_size': (2, 4)\n",
    "        }\n",
    "\n",
    "# Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=train_test_model, pbounds=pbounds, random_state=4, verbose=2)\n",
    "optimizer.maximize(init_points=5, n_iter=5)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
