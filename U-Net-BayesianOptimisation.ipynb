{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net with Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.util import random_noise\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "from keras import layers, metrics\n",
    "\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment setup and the HParams experiment summary\n",
    "\n",
    "Experiment with three hyperparameters in the model:\n",
    "\n",
    "1. Number of channels (1x or 2x)\n",
    "2. Learning rate\n",
    "3. Batch size\n",
    "4. Epochs\n",
    "5. Picture size\n",
    "6. (Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Function to create and train CNN model\n",
    "def cnn_model(learning_rate, num_filters, kernel_size, pool_size):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_filters, kernel_size, activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=128, verbose=0, validation_split=0.2)\n",
    "    val_acc = history.history['val_accuracy'][-1]  # Use validation accuracy as the metric to maximize\n",
    "    return val_acc\n",
    "\n",
    "# Define parameter space\n",
    "pbounds = {'learning_rate': (1e-5, 1e-2),\n",
    "           'num_filters': (16, 128),\n",
    "           'kernel_size': (3, 5),\n",
    "           'pool_size': (2, 4)}\n",
    "\n",
    "# Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=cnn_model, pbounds=pbounds, random_state=42)\n",
    "optimizer.maximize(init_points=10, n_iter=10)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"24_04_20\"\n",
    "\n",
    "HP_CHANNELS = hp.HParam('channels', hp.Discrete([2]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.0005])) #hp.RealInterval(0.0001, 0.01))  \n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([100]))\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([100]))\n",
    "HP_IMAGE_SIZE = hp.HParam('image_size', hp.Discrete([1]))\n",
    "HP_DATA_AUGMENTATION = hp.HParam('data_augmentation', hp.Discrete([0]))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discret\n",
    "\n",
    "METRIC_F1SCORE = 'f1_score'\n",
    "\n",
    "with tf.summary.create_file_writer(f'logs/hparam_tuning_{date}').as_default(): #_24_04_17\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_CHANNELS, HP_LEARNING_RATE, HP_BATCH_SIZE, HP_EPOCHS, HP_IMAGE_SIZE, HP_DATA_AUGMENTATION],\n",
    "    metrics=[hp.Metric(METRIC_F1SCORE, display_name='Jaccard Score')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    if i == 1:\n",
    "      plt.imshow(display_list[i], cmap='gray',  interpolation='nearest')\n",
    "    elif i == 2:\n",
    "      plt.imshow(display_list[i], cmap='jet',  interpolation='nearest')\n",
    "    else:\n",
    "      plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, session_num, date):\n",
    "    #image dimensions, seems to work with non-square inputs\n",
    "    IMAGE_CHANNELS = 3\n",
    "\n",
    "    IMAGE_HEIGHT =  192*hparams[HP_IMAGE_SIZE]\n",
    "    IMAGE_WIDTH = 64*hparams[HP_IMAGE_SIZE]\n",
    "\n",
    "    seed = 4\n",
    "    np.random.seed = seed\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    DATA_TRAIN = \"./datasets/KolektorSDD2/train/\"\n",
    "    DATA_TEST = \"./datasets/KolektorSDD2/test/\"\n",
    "\n",
    "\n",
    "\n",
    "    train_ids = next(os.walk(os.path.join(DATA_TRAIN, \"images/\")))[2]\n",
    "    test_ids = next(os.walk(os.path.join(DATA_TEST, \"images/\")))[2]\n",
    "\n",
    "    damaged = [] # prepare for data augmentation\n",
    "    damaged_mask = []\n",
    "\n",
    "    X_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "    y_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "    print('Resizing training images and masks')\n",
    "\n",
    "    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n",
    "        path = DATA_TRAIN \n",
    "\n",
    "        img = imread(path + 'images/' + id_)[:,:,:IMAGE_CHANNELS]  \n",
    "        img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        img /= 255.0\n",
    "        X_train[n] = img  #Fill empty X_train with values from img\n",
    "\n",
    "        mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "        mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "        mask = imread(mask_file)[:,:]\n",
    "\n",
    "        mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        mask /= 255.0  \n",
    "        mask = np.where(mask > 0.5, 1.0, 0.0) \n",
    "        y_train[n] = mask \n",
    "\n",
    "        if np.count_nonzero(mask) != 0:\n",
    "            damaged.append(img)\n",
    "            damaged_mask.append(mask)\n",
    "        \n",
    "    # test images\n",
    "    test_images = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "    test_masks = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "    sizes_test = []\n",
    "    print('Resizing test images') \n",
    "    for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "        path = DATA_TEST\n",
    "        img = imread(path + '/images/' + id_ )[:,:,:IMAGE_CHANNELS]\n",
    "        sizes_test.append([img.shape[0], img.shape[1]])\n",
    "        img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        img /= 255.0\n",
    "        test_images[n] = img\n",
    "\n",
    "        mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "        mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "        mask = imread(mask_file)[:,:]\n",
    "\n",
    "        mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        mask /= 255.0     \n",
    "        mask = np.where(mask > 0.5, 1.0, 0.0)   \n",
    "        test_masks[n] = mask \n",
    "    \n",
    "    ## Data augmentation - rotate and flip images\n",
    "    vertical_train = np.flip(damaged, axis=0)\n",
    "    vertical_test = np.flip(damaged_mask, axis=0)\n",
    "\n",
    "    horizontal_train = np.flip(damaged, axis=1)\n",
    "    horizontal_test = np.flip(damaged_mask, axis=1)\n",
    "\n",
    "    rotating_train = np.rot90(damaged, k=2)\n",
    "    rotating_test = np.rot90(damaged_mask, k=2)\n",
    "\n",
    "    vert_rot_train = np.rot90(vertical_train, k=2)\n",
    "    vert_rot_test = np.rot90(vertical_test, k=2)\n",
    "\n",
    "    hor_rot_train = np.rot90(horizontal_train, k=2)\n",
    "    hor_rot_test = np.rot90(horizontal_test, k=2)\n",
    "\n",
    "    # Done with rotation\n",
    "    Boxes = []\n",
    "    check = []\n",
    "    for img in damaged_mask:\n",
    "        labels = label(img)\n",
    "        regions = regionprops(labels)\n",
    "        if len(regions) == 1:\n",
    "            check.append(1)\n",
    "            for props in regions:\n",
    "                min_x, min_y, max_x, max_y = props.bbox\n",
    "                Boxes.append((min_x, min_y, max_x, max_y))\n",
    "        else:\n",
    "            check.append(0)\n",
    "\n",
    "    # Throw out images, which have more than one damage\n",
    "    onedamage = [damaged[i] for i in range(len(damaged)) if check[i] == 1]\n",
    "    onedamage_mask = [damaged_mask[i] for i in range(len(damaged_mask)) if check[i] == 1]\n",
    "    \n",
    "    def crop_image(image, bbox):\n",
    "        # Crop the image using NumPy array slicing\n",
    "        cropped_image = image[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "        return cropped_image\n",
    "\n",
    "    def overlay_image(background, background_mask, overlay, overlay_mask):\n",
    "        # Generate random position for overlay image\n",
    "        if overlay.shape[1] < background.shape[1]:\n",
    "            x_offset = np.random.randint(0, background.shape[1] - overlay.shape[1])\n",
    "            y_offset = np.random.randint(0, background.shape[0] - overlay.shape[0])\n",
    "        \n",
    "            # Overlay the image\n",
    "            background[y_offset:y_offset + overlay.shape[0], x_offset:x_offset + overlay.shape[1]] = overlay\n",
    "            background_mask[y_offset:y_offset + overlay_mask.shape[0], x_offset:x_offset + overlay_mask.shape[1]] = overlay_mask\n",
    "            return background, background_mask\n",
    "        else:\n",
    "            return background, background_mask\n",
    "\n",
    "    generated_img = np.empty((len(onedamage),IMAGE_HEIGHT,IMAGE_WIDTH,3))\n",
    "    generated_mask = np.empty((len(onedamage), IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "    overlayed_indices = []\n",
    "    for i, image in enumerate(onedamage):\n",
    "        # Get the bounding box for the current image\n",
    "        bbox = Boxes[i]\n",
    "        mask = onedamage_mask[i]\n",
    "        # Crop the image\n",
    "        cropped_image = crop_image(image, bbox)\n",
    "        cropped_mask = crop_image(mask, bbox)\n",
    "        # Pick a random overlay image from the list\n",
    "        overlay_image_index = np.random.choice([idx for idx in range(len(X_train)) if idx not in overlayed_indices])\n",
    "        overlay = X_train[overlay_image_index]\n",
    "        overlay_mask = y_train[overlay_image_index]\n",
    "\n",
    "        # Overlay the cropped image onto the random overlay image\n",
    "        new_image, new_mask = overlay_image(overlay, overlay_mask, cropped_image, cropped_mask)\n",
    "        generated_img[i] = new_image\n",
    "        generated_mask[i] = new_mask\n",
    "        overlayed_indices.append(overlay_image_index)\n",
    "    \n",
    "    noised = np.empty_like(damaged)\n",
    "    noised_mask = damaged_mask\n",
    "    i = 0\n",
    "    for img in damaged:\n",
    "        noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "        noised[i] = noise\n",
    "        i = i+1\n",
    "\n",
    "    noised_vert = np.empty_like(damaged)\n",
    "    noised_mask_vert = vertical_test\n",
    "    i = 0\n",
    "    for img in vertical_train:\n",
    "        noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "        noised_vert[i] = noise\n",
    "        i = i+1\n",
    "\n",
    "    noised_horr = np.empty_like(damaged)\n",
    "    noised_mask_horr = horizontal_test\n",
    "    i = 0\n",
    "    for img in horizontal_train:\n",
    "        noise = random_noise(img, mode='gaussian', rng=seed, clip=True)\n",
    "        noised_horr[i] = noise\n",
    "        i = i+1\n",
    "\n",
    "    if hparams[HP_DATA_AUGMENTATION] == 0:        # Base dataset (without any augmentation)\n",
    "        pass\n",
    "    elif hparams[HP_DATA_AUGMENTATION] == 1:        # Base dataset (with rotated, flipped and original images)\n",
    "        X_train = np.concatenate((vertical_train, horizontal_train, rotating_train, vert_rot_train, hor_rot_train, X_train))\n",
    "        y_train = np.concatenate((vertical_test, horizontal_test, rotating_test, vert_rot_test, hor_rot_test, y_train))\n",
    "    elif hparams[HP_DATA_AUGMENTATION] == 2:      # Base dataset + noised images\n",
    "        X_train = np.concatenate((vertical_train, horizontal_train, rotating_train, vert_rot_train, hor_rot_train, X_train, noised, noised_vert, noised_horr))\n",
    "        y_train = np.concatenate((vertical_test, horizontal_test, rotating_test, vert_rot_test, hor_rot_test, y_train, noised_mask, noised_mask_vert, noised_mask_horr))\n",
    "    elif hparams[HP_DATA_AUGMENTATION] == 3:      # Base dataset + generated images \n",
    "        X_train = np.concatenate((vertical_train, horizontal_train, rotating_train, vert_rot_train, hor_rot_train, X_train, generated_img))\n",
    "        y_train = np.concatenate((vertical_test, horizontal_test, rotating_test, vert_rot_test, hor_rot_test, y_train, generated_mask))\n",
    "    elif hparams[HP_DATA_AUGMENTATION] == 4:      # Base dataset + noised and generated images\n",
    "        X_train = np.concatenate((vertical_train, horizontal_train, rotating_train, vert_rot_train, hor_rot_train, X_train, noised, noised_vert, noised_horr, generated_img))\n",
    "        y_train = np.concatenate((vertical_test, horizontal_test, rotating_test, vert_rot_test, hor_rot_test, y_train, noised_mask, noised_mask_vert, noised_mask_horr, generated_mask))\n",
    "\n",
    "\n",
    "    print('Dataset is ready')\n",
    "\n",
    "    non_zero = np.count_nonzero(y_train)\n",
    "    print(non_zero)\n",
    "    # print(non_zero/(IMAGE_HEIGHT*IMAGE_WIDTH*len(y_train))*100)\n",
    "\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"Percentage of faulty images in train data:\", counts[1]/(counts[0]+counts[1])*100, \" %\")\n",
    "    neg = counts[0]\n",
    "    pos = counts[1]\n",
    "   \n",
    "    initial_bias = np.log([pos/neg])\n",
    "    output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = seed)\n",
    "    \n",
    "\n",
    "    #Building U-net model\n",
    "    #Downward stream\n",
    "    inputs = tf.keras.layers.Input((IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
    "    conv_11 = layers.Conv2D(16*hparams[HP_CHANNELS],kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(inputs)\n",
    "    conv_12 = layers.Conv2D(16*hparams[HP_CHANNELS],kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_11)#TODO: Understand parameters\n",
    "\n",
    "    max_pool_1 = layers.MaxPool2D((2,2))(conv_12)\n",
    "    conv_21 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu',padding= 'same',kernel_initializer = 'he_normal')(max_pool_1)\n",
    "    conv_22 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_21)\n",
    "\n",
    "    max_pool_2 = layers.MaxPool2D((2,2))(conv_22)\n",
    "    conv_31 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_2)\n",
    "    conv_32 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_31)\n",
    "\n",
    "    max_pool_3 = layers.MaxPool2D((2,2))(conv_32)\n",
    "    conv_41 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_3)\n",
    "    conv_42 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same')(conv_41)\n",
    "\n",
    "    max_pool_4 = layers.MaxPool2D((2,2))(conv_42)\n",
    "    conv_51 = layers.Conv2D(256*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_4)\n",
    "    conv_52 = layers.Conv2D(256*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_51)\n",
    "\n",
    "    #Upward stream\n",
    "    upconv_1 = layers.Conv2DTranspose(128*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_52)\n",
    "    upconv_1_conc = layers.concatenate([upconv_1,conv_42])\n",
    "    conv_61 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_1_conc)\n",
    "    conv_62 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_61)\n",
    "\n",
    "    upconv_2 = layers.Conv2DTranspose(64*hparams[HP_CHANNELS], (2,2), strides = (2,2))(conv_62)\n",
    "    upconv_2_conc = layers.concatenate([upconv_2, conv_32])\n",
    "    conv_71 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_2_conc)\n",
    "    conv_72 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_71)\n",
    "\n",
    "    upconv_3 = layers.Conv2DTranspose(32*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_72)\n",
    "    upconv_3_conc = layers.concatenate([upconv_3,conv_22])\n",
    "    conv_81 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_3_conc)\n",
    "    conv_82 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_81)\n",
    "\n",
    "    upconv_4 = layers.Conv2DTranspose(16*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_82)\n",
    "    upconv_4_conc = layers.concatenate([upconv_4,conv_12])\n",
    "    conv_91 = layers.Conv2D(16*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_4_conc)\n",
    "    conv_92 = layers.Conv2D(16*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_91)\n",
    "    outputs = layers.Conv2D(1,(1,1), activation = 'sigmoid', padding = 'same',kernel_initializer = 'he_normal', bias_initializer=output_bias)(conv_92)#TODO: Check function here\n",
    "\n",
    "    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "    from keras.optimizers import Adam, SGD\n",
    "    optimizer = Adam(learning_rate = hparams[HP_LEARNING_RATE])\n",
    "    # optimizerSGD = SGD(learning_rate = hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    #Compiling model\n",
    "    model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=[metrics.BinaryIoU(threshold=0.5)]) #TODO: Parameters check #metrics.BinaryIoU(), 'accuracy'\n",
    "    # model.compile(optimizer=optimizerSGD, loss='mse', metrics=[metrics.BinaryIoU(\n",
    "                                                            # target_class_ids=[0],\n",
    "                                                            # threshold=0.5)]) #TODO: Parameters check #metrics.BinaryIoU(), 'accuracy'\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, restore_best_weights=True)\n",
    "\n",
    "    epochs = hparams[HP_EPOCHS]  \n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "\n",
    "    run_name = \"/run-%d\" % session_num\n",
    "    logdir = f\"logs/hparam_tuning_{date}/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + run_name\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (X_val, y_val), \n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        callbacks=[early_stopping, tensorboard_callback],\n",
    "                        batch_size = batch_size,) \n",
    "\n",
    "    # Y_pred = model.predict(test_images)\n",
    "    \n",
    "    # Model Loss\n",
    "    plot_folder = f\"./plots/{date}\"\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"],label = \"Train Loss\", color = \"black\")\n",
    "    plt.plot(history.history[\"val_loss\"],label = \"Validation Loss\", color = \"darkred\", marker = \"+\", linestyle=\"dashed\", markeredgecolor = \"purple\", markeredgewidth = 2)\n",
    "    plt.title(f\"Model Loss - session {session_num}\", color = \"darkred\", size = 13)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"plots/{date}/loss_session_{session_num}.png\")\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    model_folder = f\"./trainedModels/hptuning_{date}\"\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    model.save(f\"trainedModels/hptuning_{date}/hptuning_session{session_num}.h5\")\n",
    "\n",
    "    # Model Jaccard score\n",
    "    threshold_list = np.linspace(0.0, 0.4, 30)\n",
    "    jaccard_scores = []\n",
    "    Y_pred = model.predict(test_images)\n",
    "    true_masks_flat = test_masks.reshape(test_masks.shape[0], -1)\n",
    "    for threshold in threshold_list:\n",
    "        y_pred_binary = (Y_pred >= threshold).astype(int)\n",
    "        pred_masks_binary_flat = y_pred_binary.reshape(y_pred_binary.shape[0], -1)\n",
    "        jaccard_scores.append(jaccard_score(true_masks_flat, pred_masks_binary_flat, average=\"micro\"))\n",
    "    print(\"Best jacard score: \", max(jaccard_scores))\n",
    "    \n",
    "    accuracy = max(jaccard_scores)\n",
    "\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams, session_num, date):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams, session_num, date)\n",
    "    tf.summary.scalar(METRIC_F1SCORE, accuracy, step=1)\n",
    "    tf.summary.scalar('Run nr.', session_num, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-4\n",
      "{'channels': 2, 'image_size': 1, 'learning_rate': 0.0005, 'batch_size': 100, 'epochs': 100, 'data_augmentation': 0}\n",
      "Resizing training images and masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2332 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2332/2332 [01:12<00:00, 32.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing test images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004/1004 [00:31<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is ready\n",
      "148516\n",
      "{0.0: 28507100, 1.0: 148516}\n",
      "Percentage of faulty images in train data: 0.5182788602415666  %\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 192, 64, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 192, 64, 32)  896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 192, 64, 32)  9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 96, 32, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 96, 32, 64)   18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 96, 32, 64)   36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 48, 16, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 48, 16, 128)  73856       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 48, 16, 128)  147584      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 24, 8, 128)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 24, 8, 256)   295168      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 24, 8, 256)   590080      ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 12, 4, 256)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 12, 4, 512)   1180160     ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 12, 4, 512)   2359808     ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 24, 8, 256)  524544      ['conv2d_9[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 24, 8, 512)   0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 24, 8, 256)   1179904     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 24, 8, 256)   590080      ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 48, 16, 128)  131200     ['conv2d_11[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 48, 16, 256)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 48, 16, 128)  295040      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 48, 16, 128)  147584      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 96, 32, 64)  32832       ['conv2d_13[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 96, 32, 128)  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 96, 32, 64)   73792       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 96, 32, 64)   36928       ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 192, 64, 32)  8224       ['conv2d_15[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 192, 64, 64)  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 192, 64, 32)  18464       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 192, 64, 32)  9248        ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 192, 64, 1)   33          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,760,097\n",
      "Trainable params: 7,760,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 67s 1s/step - loss: 0.0339 - binary_io_u: 0.4972 - val_loss: 0.0286 - val_binary_io_u: 0.4977\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 27s 998ms/step - loss: 0.0309 - binary_io_u: 0.5251 - val_loss: 0.0286 - val_binary_io_u: 0.4977\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 18s 986ms/step - loss: 0.0312 - binary_io_u: 0.5099 - val_loss: 0.0225 - val_binary_io_u: 0.6001\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 15s 962ms/step - loss: 0.0250 - binary_io_u: 0.5677 - val_loss: 0.0189 - val_binary_io_u: 0.6152\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 15s 972ms/step - loss: 0.0277 - binary_io_u: 0.5726 - val_loss: 0.0214 - val_binary_io_u: 0.5693\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 14s 927ms/step - loss: 0.0253 - binary_io_u: 0.5686 - val_loss: 0.0227 - val_binary_io_u: 0.5467\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 14s 887ms/step - loss: 0.0192 - binary_io_u: 0.5998 - val_loss: 0.0162 - val_binary_io_u: 0.6511\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 14s 931ms/step - loss: 0.0171 - binary_io_u: 0.6320 - val_loss: 0.0138 - val_binary_io_u: 0.6329\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 15s 984ms/step - loss: 0.0167 - binary_io_u: 0.6300 - val_loss: 0.0161 - val_binary_io_u: 0.6988\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 15s 985ms/step - loss: 0.0141 - binary_io_u: 0.6747 - val_loss: 0.0118 - val_binary_io_u: 0.6947\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 15s 978ms/step - loss: 0.0119 - binary_io_u: 0.6728 - val_loss: 0.0123 - val_binary_io_u: 0.6577\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 15s 977ms/step - loss: 0.0122 - binary_io_u: 0.7009 - val_loss: 0.0116 - val_binary_io_u: 0.6945\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 18s 991ms/step - loss: 0.0143 - binary_io_u: 0.6748 - val_loss: 0.0124 - val_binary_io_u: 0.7082\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 15s 979ms/step - loss: 0.0109 - binary_io_u: 0.7097 - val_loss: 0.0141 - val_binary_io_u: 0.6954\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 15s 965ms/step - loss: 0.0133 - binary_io_u: 0.6904 - val_loss: 0.0117 - val_binary_io_u: 0.7199\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 15s 977ms/step - loss: 0.0102 - binary_io_u: 0.7296 - val_loss: 0.0128 - val_binary_io_u: 0.6717\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 15s 996ms/step - loss: 0.0105 - binary_io_u: 0.7257 - val_loss: 0.0131 - val_binary_io_u: 0.7391\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 15s 989ms/step - loss: 0.0104 - binary_io_u: 0.7200 - val_loss: 0.0114 - val_binary_io_u: 0.7449\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 15s 1s/step - loss: 0.0097 - binary_io_u: 0.7417 - val_loss: 0.0107 - val_binary_io_u: 0.7314\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 15s 968ms/step - loss: 0.0090 - binary_io_u: 0.7519 - val_loss: 0.0098 - val_binary_io_u: 0.7208\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 15s 975ms/step - loss: 0.0075 - binary_io_u: 0.7784 - val_loss: 0.0090 - val_binary_io_u: 0.7599\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 15s 985ms/step - loss: 0.0072 - binary_io_u: 0.7867 - val_loss: 0.0105 - val_binary_io_u: 0.7555\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 18s 991ms/step - loss: 0.0073 - binary_io_u: 0.8069 - val_loss: 0.0080 - val_binary_io_u: 0.7588\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 12s 791ms/step - loss: 0.0065 - binary_io_u: 0.8095 - val_loss: 0.0092 - val_binary_io_u: 0.7539\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 15s 798ms/step - loss: 0.0057 - binary_io_u: 0.8313 - val_loss: 0.0091 - val_binary_io_u: 0.7635\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 15s 796ms/step - loss: 0.0055 - binary_io_u: 0.8365 - val_loss: 0.0084 - val_binary_io_u: 0.7709\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 15s 788ms/step - loss: 0.0052 - binary_io_u: 0.8381 - val_loss: 0.0092 - val_binary_io_u: 0.7582\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 14s 718ms/step - loss: 0.0055 - binary_io_u: 0.8215 - val_loss: 0.0096 - val_binary_io_u: 0.7669\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 13s 724ms/step - loss: 0.0057 - binary_io_u: 0.8331 - val_loss: 0.0079 - val_binary_io_u: 0.7907\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 15s 802ms/step - loss: 0.0045 - binary_io_u: 0.8401 - val_loss: 0.0102 - val_binary_io_u: 0.7534\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 15s 811ms/step - loss: 0.0064 - binary_io_u: 0.8154 - val_loss: 0.0085 - val_binary_io_u: 0.7767\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 15s 799ms/step - loss: 0.0054 - binary_io_u: 0.8287 - val_loss: 0.0096 - val_binary_io_u: 0.7503\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0054 - binary_io_u: 0.8332 - val_loss: 0.0106 - val_binary_io_u: 0.7738\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 15s 991ms/step - loss: 0.0047 - binary_io_u: 0.8531 - val_loss: 0.0094 - val_binary_io_u: 0.7817\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 15s 993ms/step - loss: 0.0043 - binary_io_u: 0.8621 - val_loss: 0.0103 - val_binary_io_u: 0.7562\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 15s 986ms/step - loss: 0.0040 - binary_io_u: 0.8659 - val_loss: 0.0098 - val_binary_io_u: 0.7768\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 15s 978ms/step - loss: 0.0036 - binary_io_u: 0.8678 - val_loss: 0.0100 - val_binary_io_u: 0.7713\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 15s 981ms/step - loss: 0.0036 - binary_io_u: 0.8860 - val_loss: 0.0106 - val_binary_io_u: 0.7690\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 15s 989ms/step - loss: 0.0031 - binary_io_u: 0.8847 - val_loss: 0.0109 - val_binary_io_u: 0.7762\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 15s 980ms/step - loss: 0.0029 - binary_io_u: 0.8947 - val_loss: 0.0105 - val_binary_io_u: 0.7835\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 15s 982ms/step - loss: 0.0028 - binary_io_u: 0.9061 - val_loss: 0.0119 - val_binary_io_u: 0.7691\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 15s 977ms/step - loss: 0.0029 - binary_io_u: 0.8979 - val_loss: 0.0109 - val_binary_io_u: 0.7789\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 15s 979ms/step - loss: 0.0028 - binary_io_u: 0.8988 - val_loss: 0.0099 - val_binary_io_u: 0.7877\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0029 - binary_io_u: 0.8935Restoring model weights from the end of the best epoch: 29.\n",
      "16/16 [==============================] - 15s 1s/step - loss: 0.0029 - binary_io_u: 0.8935 - val_loss: 0.0125 - val_binary_io_u: 0.7584\n",
      "Epoch 44: early stopping\n",
      "32/32 [==============================] - 4s 131ms/step\n",
      "Best jacard score:  0.6066071358633924\n"
     ]
    }
   ],
   "source": [
    "session_num = 4\n",
    "for channels in HP_CHANNELS.domain.values:\n",
    "  for image_size in HP_IMAGE_SIZE.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "      for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "        for epochs in HP_EPOCHS.domain.values:\n",
    "          for data_augmentation in HP_DATA_AUGMENTATION.domain.values:\n",
    "            hparams = {\n",
    "                HP_CHANNELS: channels,\n",
    "                HP_IMAGE_SIZE: image_size,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_BATCH_SIZE: batch_size,\n",
    "                HP_EPOCHS: epochs,\n",
    "                HP_DATA_AUGMENTATION: data_augmentation,\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run(f\"logs/hparam_tuning_{date}/{run_name}\", hparams, session_num, date)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning_24_04_18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
