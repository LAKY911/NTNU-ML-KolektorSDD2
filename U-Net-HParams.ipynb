{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net by Lukas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import layers, metrics\n",
    "\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment setup and the HParams experiment summary\n",
    "\n",
    "Experiment with three hyperparameters in the model:\n",
    "\n",
    "1. Number of channels (1x or 2x)\n",
    "2. Learning rate\n",
    "3. Batch size\n",
    "4. Epochs\n",
    "5. Picture size\n",
    "6. (Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_CHANNELS = hp.HParam('channels', hp.Discrete([1, 2]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.0001, 0.01))  \n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32, 64]))\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([10, 20]))\n",
    "HP_IMAGE_SIZE = hp.HParam('image_size', hp.Discrete([1, 2]))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_F1SCORE = 'f1_score'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_CHANNELS, HP_LEARNING_RATE, HP_BATCH_SIZE, HP_EPOCHS, HP_IMAGE_SIZE],\n",
    "    metrics=[hp.Metric(METRIC_F1SCORE, display_name='F1-score')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    if i == 1:\n",
    "      plt.imshow(display_list[i], cmap='gray',  interpolation='nearest')\n",
    "    elif i == 2:\n",
    "      plt.imshow(display_list[i], cmap='jet',  interpolation='nearest')\n",
    "    else:\n",
    "      plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    #image dimensions, seems to work with non-square inputs\n",
    "    IMAGE_CHANNELS = 3\n",
    "\n",
    "    IMAGE_HEIGHT =  192*hparams[HP_IMAGE_SIZE]\n",
    "    IMAGE_WIDTH = 64*hparams[HP_IMAGE_SIZE]\n",
    "\n",
    "    seed = 4\n",
    "    np.random.seed = seed\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    DATA_TRAIN = \"./datasets/KolektorSDD2/train/\"\n",
    "    DATA_TEST = \"./datasets/KolektorSDD2/test/\"\n",
    "\n",
    "\n",
    "\n",
    "    train_ids = next(os.walk(os.path.join(DATA_TRAIN, \"images/\")))[2]\n",
    "    test_ids = next(os.walk(os.path.join(DATA_TEST, \"images/\")))[2]\n",
    "\n",
    "\n",
    "    X_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "    y_train = np.zeros((len(train_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "    print('Resizing training images and masks')\n",
    "\n",
    "    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \n",
    "        path = DATA_TRAIN \n",
    "\n",
    "        img = imread(path + 'images/' + id_)[:,:,:IMAGE_CHANNELS]  \n",
    "        img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        img /= 255.0\n",
    "        X_train[n] = img  #Fill empty X_train with values from img\n",
    "\n",
    "        mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "        mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "        mask = imread(mask_file)[:,:]\n",
    "\n",
    "        mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        mask /= 255.0  \n",
    "        mask = np.where(mask > 0.5, 1.0, 0.0) \n",
    "        y_train[n] = mask \n",
    "        \n",
    "    # test images\n",
    "    test_images = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.float16)\n",
    "    test_masks = np.zeros((len(test_ids), IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.float16)\n",
    "\n",
    "    sizes_test = []\n",
    "    print('Resizing test images') \n",
    "    for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "        path = DATA_TEST\n",
    "        img = imread(path + '/images/' + id_ )[:,:,:IMAGE_CHANNELS]\n",
    "        sizes_test.append([img.shape[0], img.shape[1]])\n",
    "        img = resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        img /= 255.0\n",
    "        test_images[n] = img\n",
    "\n",
    "        mask = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=bool)\n",
    "        mask_file = os.path.join(path + 'masks/' + id_[:5] + \"_GT.png\")\n",
    "        mask = imread(mask_file)[:,:]\n",
    "\n",
    "        mask = resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH), mode='constant', preserve_range=True)\n",
    "        mask /= 255.0     \n",
    "        mask = np.where(mask > 0.5, 1.0, 0.0)   \n",
    "        test_masks[n] = mask \n",
    "\n",
    "    print('Dataset is ready')\n",
    "\n",
    "    non_zero = np.count_nonzero(y_train)\n",
    "    print(non_zero)\n",
    "    # print(non_zero/(IMAGE_HEIGHT*IMAGE_WIDTH*len(y_train))*100)\n",
    "\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"Percentage of faulty images in train data:\", counts[1]/(counts[0]+counts[1])*100, \" %\")\n",
    "    neg = counts[0]\n",
    "    pos = counts[1]\n",
    "   \n",
    "    initial_bias = np.log([pos/neg])\n",
    "    output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = seed)\n",
    "    \n",
    "\n",
    "    #Building U-net model\n",
    "    #Downward stream\n",
    "    inputs = tf.keras.layers.Input((IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
    "    conv_11 = layers.Conv2D(16*hparams[HP_CHANNELS],kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(inputs)\n",
    "    conv_12 = layers.Conv2D(16*hparams[HP_CHANNELS],kernel_size=(3,3), activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_11)#TODO: Understand parameters\n",
    "\n",
    "    max_pool_1 = layers.MaxPool2D((2,2))(conv_12)\n",
    "    conv_21 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu',padding= 'same',kernel_initializer = 'he_normal')(max_pool_1)\n",
    "    conv_22 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_21)\n",
    "\n",
    "    max_pool_2 = layers.MaxPool2D((2,2))(conv_22)\n",
    "    conv_31 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_2)\n",
    "    conv_32 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_31)\n",
    "\n",
    "    max_pool_3 = layers.MaxPool2D((2,2))(conv_32)\n",
    "    conv_41 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_3)\n",
    "    conv_42 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same')(conv_41)\n",
    "\n",
    "    max_pool_4 = layers.MaxPool2D((2,2))(conv_42)\n",
    "    conv_51 = layers.Conv2D(256*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(max_pool_4)\n",
    "    conv_52 = layers.Conv2D(256*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding= 'same',kernel_initializer = 'he_normal')(conv_51)\n",
    "\n",
    "    #Upward stream\n",
    "    upconv_1 = layers.Conv2DTranspose(128*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_52)\n",
    "    upconv_1_conc = layers.concatenate([upconv_1,conv_42])\n",
    "    conv_61 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_1_conc)\n",
    "    conv_62 = layers.Conv2D(128*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_61)\n",
    "\n",
    "    upconv_2 = layers.Conv2DTranspose(64*hparams[HP_CHANNELS], (2,2), strides = (2,2))(conv_62)\n",
    "    upconv_2_conc = layers.concatenate([upconv_2, conv_32])\n",
    "    conv_71 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_2_conc)\n",
    "    conv_72 = layers.Conv2D(64*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_71)\n",
    "\n",
    "    upconv_3 = layers.Conv2DTranspose(32*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_72)\n",
    "    upconv_3_conc = layers.concatenate([upconv_3,conv_22])\n",
    "    conv_81 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_3_conc)\n",
    "    conv_82 = layers.Conv2D(32*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_81)\n",
    "\n",
    "    upconv_4 = layers.Conv2DTranspose(16*hparams[HP_CHANNELS],(2,2), strides=(2,2))(conv_82)\n",
    "    upconv_4_conc = layers.concatenate([upconv_4,conv_12])\n",
    "    conv_91 = layers.Conv2D(16*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(upconv_4_conc)\n",
    "    conv_92 = layers.Conv2D(16*hparams[HP_CHANNELS],(3,3),activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv_91)\n",
    "    outputs = layers.Conv2D(1,(1,1), activation = 'sigmoid', padding = 'same',kernel_initializer = 'he_normal', bias_initializer=output_bias)(conv_92)#TODO: Check function here\n",
    "\n",
    "    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "    from keras.optimizers import Adam\n",
    "    optimizer = Adam(learning_rate = hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    #Compiling model\n",
    "    model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=['f1_score']) #TODO: Parameters check #metrics.BinaryIoU()\n",
    "    model.summary()\n",
    "\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, restore_best_weights=True)\n",
    "\n",
    "    epochs = hparams[HP_EPOCHS]  \n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs = epochs, \n",
    "                        validation_data = (X_val, y_val), \n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        callbacks=[early_stopping]) \n",
    "\n",
    "    # Y_pred = model.predict(test_images)\n",
    "\n",
    "    model.save(f\"trainedModels/hptuning_session{session_num}.h5\")\n",
    "\n",
    "    # model.fit(x_train, y_train, epochs=1) # Run with 1 epoch to speed things up for demo purposes\n",
    "    _, accuracy = model.evaluate(test_images, test_masks)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams, session_num):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams, session_num)\n",
    "    tf.summary.scalar(METRIC_F1SCORE, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for channels in HP_CHANNELS.domain.values:\n",
    "  for image_size in HP_IMAGE_SIZE.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "      for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "        for epochs in HP_EPOCHS.domain.values:\n",
    "          hparams = {\n",
    "              HP_CHANNELS: channels,\n",
    "              HP_IMAGE_SIZE: image_size,\n",
    "              HP_LEARNING_RATE: learning_rate,\n",
    "              HP_BATCH_SIZE: batch_size,\n",
    "              HP_EPOCHS: epochs,\n",
    "          }\n",
    "          run_name = \"run-%d\" % session_num\n",
    "          print('--- Starting trial: %s' % run_name)\n",
    "          print({h.name: hparams[h] for h in hparams})\n",
    "          run('logs/hparam_tuning/' + run_name, hparams, session_num)\n",
    "          session_num += 1\n",
    "\n",
    "# HP_CHANNELS = hp.HParam('channels', hp.Discrete([1, 2]))\n",
    "# HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.0001, 0.01))  \n",
    "# HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32, 64]))\n",
    "# HP_EPOCHS = hp.HParam('epochs', hp.Discrete([10, 20]))\n",
    "# HP_IMAGE_SIZE = hp.HParam('image_size', hp.Discrete([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
